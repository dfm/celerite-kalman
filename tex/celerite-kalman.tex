% The layout of this document has been shamelessly stolen from the MIT licensed project:
% https://github.com/davidwhogg/GaussianProductRefactor
% by David W. Hogg, Adrian Price-Whelan, and Boris Leistedt

\documentclass[10pt]{article}

\usepackage{amsmath, bm, mathrsfs, amssymb, algorithm, algorithmicx}
\usepackage[noend]{algpseudocode}
\usepackage[dvipsnames]{xcolor}
\usepackage[hidelinks,
            colorlinks=true,
            linkcolor=NavyBlue,
            citecolor=darkgray,
            urlcolor=NavyBlue]{hyperref}
\usepackage{pifont}
\usepackage{graphicx}
\usepackage{doi}

\usepackage{natbib}
\bibliographystyle{dfm_abbrvnat}
\setcitestyle{round,citesep={,},aysep={}}

% text stuhh
\newcommand{\documentname}{\textsl{Note}}
\newcommand{\sectionname}{Section}
\newcommand{\equationname}{equation}
\newcommand{\notename}{Note}
\renewcommand{\tablename}{Table}
\newcommand{\acronym}[1]{{\small{#1}}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\foreign}[1]{\textsl{#1}}
\newcommand{\etal}{\foreign{et~al.}}

% math stuhh
\newcommand{\hquad}{~~}
\newcommand{\given}{\,|\,}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\T}{^{\!\mathsf{T}\!}}
\newcommand{\inv}{^{-1}}
\newcommand{\scalar}[1]{#1}
\renewcommand{\vector}[1]{\boldsymbol{#1}}
\newcommand{\tensor}[1]{\mathbf{#1}}
\renewcommand{\matrix}[1]{\mathsf{#1}}
\newcommand{\normal}{\mathcal{N}\!\,}

% variables
\newcommand{\celerite}{\code{celerite}}

% Journal keys - kill me now
\newcommand{\aj}{Astron.\,J.}
\newcommand{\apj}{Astrophys.\,J.} % Astrophysical Journal
\newcommand{\apjs}{Astrophys.\,J.\,Supp.\,Ser.} % Astrophysical Journal Supplements

% page layout stuff
\setlength{\topmargin}{0.0in}
\setlength{\headheight}{0.0in}
\setlength{\headsep}{0.0in}
\setlength{\parindent}{\baselineskip}
\setlength{\textwidth}{4.3in}
\setlength{\textheight}{2\textwidth}
\raggedbottom\sloppy\sloppypar\frenchspacing

% this might be crazy, but here's my number
\setlength{\marginparsep}{0.15in}
\setlength{\marginparwidth}{2.7in}
\usepackage{marginfix} % necessary but possibly evil
\newcounter{marginnote}
\setcounter{marginnote}{0}
\renewcommand{\footnote}[1]{\refstepcounter{marginnote}\textsuperscript{\themarginnote}\marginpar{\color{darkgray}\raggedright\footnotesize\textsuperscript{\themarginnote}#1}}
\newcommand{\tfigurerule}{\rule{0pt}{1ex}\\ \rule{\marginparwidth}{0.5pt}\\ \rule{0pt}{0.25ex}}
\newcommand{\bfigurerule}{\rule{0pt}{0.25ex}\\ \rule{\marginparwidth}{0.5pt}\\ \rule{0pt}{1ex}}
\renewcommand{\caption}[1]{\parbox{\marginparwidth}{\footnotesize\refstepcounter{figure}\textbf{\figurename~\thefigure}: {#1}}}

% and make the left margin correct
\setlength{\oddsidemargin}{0.5\paperwidth}
\addtolength{\oddsidemargin}{-1.0in}
\addtolength{\oddsidemargin}{-0.5\textwidth}
\addtolength{\oddsidemargin}{-0.5\marginparwidth}
\addtolength{\oddsidemargin}{-0.5\marginparsep}

\begin{document}

\section*{The relationship between celerite \& state-space models\footnote{%
    The methods described here are available as part of the \href{https://github.com/exoplanet-dev/celerite2}{\code{exoplanet-dev/celerite2} project on GitHub} (CITE ZENODO) and the code used to make the figures for this paper are available as part of the \href{https://github.com/dfm/latent-celerite-process-paper}{\code{dfm/latent-celerite-process-paper} project on GitHub} (CITE ZENODO).}}

\noindent\textbf{Daniel Foreman-Mackey}\footnote{%
  The authors would like to thank the Astronomical Data Group at Flatiron for listening to every iteration of this project and for providing great feedback every step of the way.
  We would also like to thank
  Suzanne Aigrain (Oxford),
  Gregory Guida (Etsy),
  Christopher Stumm (Etsy),
  \emph{and others\ldots}
  for insightful conversations and other contributions.
}\\
{\footnotesize%
\textsl{Center for Computational Astrophysics, Flatiron Institute, New York, NY, USA}%
}

\medskip\noindent\textbf{Final author list and order TBD}\\
{\footnotesize%
\textsl{Affiliations}\\
\textsl{More affiliations}%
}

\paragraph{Abstract:}

% Probably better to re-write focusing more on the latent models instead of story time.

Matrices with semi-separable structure are at the core of some modern Gaussian Process algorithms and they allow efficient operations that are scalable in terms of computational cost and memory use.
In this note, we discuss efficient implementation of the standard matrix operations on semi-separable matrices and the forward- and reverse- mode automatic differentiation algorithms.
We also discuss the relationship with Kalman filtering methods for Gaussian Process regression.

\section{Introduction}

An $N$ by $N$, symmetric, rank-$R$ semi-separable matrix $K$ is defined as
\begin{eqnarray}
  K &=& \mathrm{diag}(A) + f(U,\, V) + f(U,\, V)\T
\end{eqnarray}
where $\mathrm{diag}(A)$ is a diagonal matrix, $f(U,\, V)$ is a lower triangular matrix, and both $U$ and $V$ are matrices with the shape $N \times R$.
The function $f(U,\, V)$ is often defined as
\begin{eqnarray}
  f(U,\, V) &=& \mathrm{tril}(U \, V\T)
\end{eqnarray}
where $\mathrm{tril}$ extracts the strict lower triangle of a square matrix, but it will be useful to allow more general definitions below.

Matrices with this structure are useful since many standard operations can be performed with $\mathcal{O}(N\,R)$ or $\mathcal{O}(N\,R^2)$ scaling for both computation and memory.

The key operations that we will discuss are:

\begin{enumerate}
  \item Matrix multiplication
  \item Cholesky factorization
  \item Triangular solves
  \item Diagonal of matrix-matrix multiplication
\end{enumerate}

We will also derive the forward- and reverse-mode differentiation rules for these operations and discuss their efficient implementation.

A paper. \citep{foreman-mackey2017,foreman-mackey2018,gordon2020}

\citep{loper2020,hu2020}

\section{Associative scan algorithm}

Following \citep{sarkka2021}.

It turns out that we can re-write all of the standard operations on semi-separable matrices as scan operations with an associative binary operator.
This is beneficial because there are well defined methods for parallelizing such algorithms and it will allow efficient parallel implementations on the GPU.

An ``associative scan'' or ``prefix sum'' operation for a sequence
\begin{eqnarray}
  (a_1,\, a_2,\, a_3,\, \ldots)
\end{eqnarray}
and binary operator $\otimes$ is defined as
\begin{eqnarray}
  (a_1,\, a_1 \otimes a_2,\, a_1 \otimes a_2 \otimes a_3,\, \ldots)\quad.
\end{eqnarray}
For example, if the operator is addition, then we would get the cumulative sum: each element becomes the sum of all earlier entries.

For semi-separable matrices, all of the operations can be written as inclusive scans like this.

\paragraph{Matrix multiply}
The algorithm for performing a lower triangular matrix multiply is listed in Figure~\ref{alg:tril-mat-mul}.
This can be rewritten as a scan operation where the state is
\begin{eqnarray}
  a_n &=& \left(\begin{array}{c}
    P_n \\
    P_n\,v_n\,{y_n}\T
  \end{array}\right)
\end{eqnarray}
and the binary operation $\otimes$ is defined as
\begin{eqnarray}
  a_n \otimes a_m &=& \left(\begin{array}{c}
    P_m\,P_n \\
    P_m\,P_n\,v_n\,{y_n}\T + P_m\,v_m\,{y_m}\T
  \end{array}\right) \quad.
\end{eqnarray}
We can demonstrate that this is an associative operation by demonstrating that
\begin{eqnarray}
  (a_n \otimes a_m) \otimes a_k &=& a_n \otimes (a_m \otimes a_k) \quad.
\end{eqnarray}

\begin{algorithm}
  \caption{Lower triangular matrix multiply\label{alg:tril-mat-mul}}
  \begin{algorithmic}[1]
    \Require{$P_n \in \mathbb{R}^{J \times J}$, $u_n \in \mathbb{R}^{J}$, $v_n \in \mathbb{R}^{J}$, and $y_n \in \mathbb{R}^{M}$}
    \Function{TrilMatMul}{$\{P_n,\, u_n,\, v_n,\, y_n\}_{n=1}^{N}$}
    \State{$F_1 \gets 0$} \Comment{$F_n \in \mathbb{R}^{J \times M}$}
    \State{$z_1 \gets 0$} \Comment{$z_n \in \mathbb{R}^{M}$}
    \For{$n \gets 2 \textrm{ to } N$}
    \State{$F_n \gets P_{n-1}\,\left(F_{n-1}+{v_{n-1}}\,{y_{n-1}}\T\right)$}
    \State{$z_n \gets {u_n}\T\,F_n$}
    \EndFor
    \State \Return{$Z$}
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\paragraph{Triangular solve}
Similarly, the algorithm for computing the lower triangular solve is provided in Figure~\ref{alg:tril-solve}.
In this case, the some what more complicated associative scan algorithm is defined by the state
\begin{eqnarray}
  a_n &=& \left(\begin{array}{c}
    A_n \\
    b_n
  \end{array}\right)
\end{eqnarray}
where
\begin{eqnarray}
  A_n &=& \left(\begin{array}{cc}
    P_{n-1}           & P_{n-1}\,v_{n-1}  \\
    -{u_n}\T\,P_{n-1} & -{u_n}\T\,P_{n-1}
  \end{array}\right) \\
  &=& \left(\begin{array}{c}
    1 \\
    -{u_n}\T
  \end{array}\right)\,\left(\begin{array}{cc}
    P_{n-1} & P_{n-1}\,v_{n-1}
  \end{array}\right)
\end{eqnarray}
and
\begin{eqnarray}
  b_n &=& \left(\begin{array}{c}
    0 \\
    {y_n}\T
  \end{array}\right)\quad.
\end{eqnarray}
In this case, the binary operator is
\begin{eqnarray}
  a_n \otimes a_m &=& \left(\begin{array}{c}
    A_m \, A_n \\
    A_m \, b_n + b_m
  \end{array}\right)\quad.
\end{eqnarray}
Both elements of $a_n \otimes a_m$ continue to have low-rank structure that can be exploited for efficient computation.

% where, for completeness, we can expand
% \begin{eqnarray}
%   A_m\,A_n &=&  \left(\begin{array}{c}
%       1 \\
%       -{u_m}\T
%     \end{array}\right)\,\left[P_{m-1}\,\left(1-v_{m-1}\,{u_{n}}\T\right)\,P_{n-1}\right]\,\left(\begin{array}{cc}
%       1 & v_{n-1}
%     \end{array}\right)
% \end{eqnarray}
% and
% \begin{eqnarray}
%   A_m\,b_n+b_m &=& \left(\begin{array}{c}
%     P_{m-1}\,v_{m-1}\,{y_n}\T \\
%     {y_m}\T-{u_{m}}\T\,P_{m-1}\,v_{m-1}\,{y_n}\T
%   \end{array}\right)\quad.
% \end{eqnarray}


\begin{algorithm}
  \caption{Lower triangular matrix solve\label{alg:tril-solve}}
  \begin{algorithmic}[1]
    \Require{$P_n \in \mathbb{R}^{J \times J}$, $u_n \in \mathbb{R}^{J}$, $v_n \in \mathbb{R}^{J}$, and $y_n \in \mathbb{R}^{M}$}
    \Function{TrilSolve}{$\{P_n,\, u_n,\, v_n,\, y_n\}_{n=1}^{N}$}
    \State{$F_1 \gets 0$} \Comment{$F_n \in \mathbb{R}^{J \times M}$}
    \State{$z_1 \gets y_n$} \Comment{$z_n \in \mathbb{R}^{M}$}
    \For{$n \gets 2 \textrm{ to } N$}
    \State{$F_n \gets P_{n-1}\,\left(F_{n-1}+{v_{n-1}}\,{z_{n-1}}\T\right)$}
    \State{$z_n \gets y_n - {u_n}\T\,F_n$}
    \EndFor
    \State \Return{$Z$}
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\section{Relationship with Kalman filtering \& state space models}

The linear Gaussian state space model model is
\begin{eqnarray}
  p(\vector{x}_n\,|\,\vector{x}_{n-1}) &=& \normal \left(\vector{x}_n;\,\matrix{P}_n\,\vector{x}_{n-1},\,\matrix{Q}_{n}\right) \\
  p(\vector{y}_n\,|\,\vector{x}_n) &=& \normal \left(\vector{y}_n;\,\matrix{H}_n\,\vector{x}_n,\,\matrix{R}_n\right)
\end{eqnarray}
where $\vector{x}_n$ denotes the ``state'' at time $t_n$ and $\vector{y}_n$ is the observation at the same time.

Such a model corresponds to the solution to the differential equation
\begin{eqnarray}
  \dot{\vector{x}} = \matrix{A}\,\vector{x} + \matrix{L}\,\vector{w}
\end{eqnarray}
where $\vector{w}$ is a noise process.

The standard (periodic) models in \celerite\ are specified with the following power spectrum:
\begin{eqnarray}
  S(\omega) &=& \sqrt{\frac{2}{\pi}}\,\frac{(a\,c+b\,d)\,(c^2 + d^2) + (a\,c - b\,d)\,\omega^2}{\omega^4 + 2\,(c^2 - d^2)\,\omega^2 + (c^2 + d^2)^2}
\end{eqnarray}
where $a$, $b$, $c$, and $d$ are the parameters of the model.
By inspection, this model corresponds to the following matrices
\begin{eqnarray}
  \matrix{H} &=& \left(\begin{array}{cc}
      \sqrt{(a\,c+b\,d)\,(c^2 + d^2)} & \sqrt{a\,c - b\,d}
    \end{array}\right) \\
  \matrix{A} &=& \left(\begin{array}{cc}
      0          & 1     \\
      -c^2 - d^2 & -2\,c
    \end{array}\right) \\
  \matrix{L} &=& \left(\begin{array}{c}
      0 \\
      1
    \end{array}\right)
\end{eqnarray}

This system can be integrated to find the $\matrix{P}$ and $\matrix{Q}$ matrices (insert maths here):
\begin{eqnarray}
  \matrix{P}(\tau) &=& \frac{e^{-c\,\tau}}{d}\,\left(\begin{array}{cc}
      d\,\cos(d\,\tau) + c\,\sin(d\,\tau) & \sin(d\,\tau)                       \\
      -(c^2 + d^2)\,\sin(d\,\tau)         & d\,\cos(d\,\tau) - c\,\sin(d\,\tau)
    \end{array}\right) \nonumber\\
  \matrix{Q}(\tau) &=& \frac{e^{-2\,c\,t}}{2\,c\,d^2}\left(\begin{array}{cc}
      Q_{11} & Q_{12} \\
      Q_{12} & Q_{22}
    \end{array}\right)
\end{eqnarray}
where
\begin{eqnarray}
  Q_{11} &=& \frac{-\sin^3(d\,\tau) + 8\,(c^2+d^2)^{-1}\,e^{c\,\tau}\,\left(d^2\,\cosh(c\,\tau)\,\sin(d\,\tau) - d^3/c\,\sinh(c\,\tau)\,\cos(d\,\tau)\right)}{c\,\sin d\,\tau - d\,\cos d\,\tau} \nonumber\\
  Q_{12} &=& c\,\sin^2 (d\,\tau) \,\nonumber\\
  Q_{22} &=& -c^2\,\sin^2 (d\,\tau) + c\,d\,\sin(d\,\tau)\,\cos(d\,\tau) + d^2\,\sinh(c\,\tau)\,e^{c\,\tau}/2 \nonumber
  % (e^{2\,c\,\tau}-1)
\end{eqnarray}

% Q[1, 0] = sin(d*t)**2
% Q[1, 1] = (-2*c**2*sin(d*t)**2 + c*d*sin(2*d*t) + d**2*exp(2*c*t) - d**2)/(2*c)
% Q[0, 0], denom = 4*c*(c**2 + d**2)*(c*sin(d*t) - d*cos(d*t))
% num = -4*c*(c**2 + d**2)*sm.sin(d*t)**3 + 2*c*d**2*(sm.exp(2*c*t) + 1)*sm.sin(d*t) - 2*d**3*(sm.exp(2*c*t) - 1)*sm.cos(d*t)

% \marginpar{\tfigurerule\\
%   \includegraphics[width=\marginparwidth, trim=0ex 0ex 0ex 0.5in, clip]{oned.pdf}
%   \caption{The one-dimensional case: If the prior pdf and the likelihood are both
%     Gaussian in a single parameter, their product (and hence the posterior pdf) is
%     also Gaussian, with a narrower width (smaller variance) than
%     either the prior pdf or the likelihood.\label{fig:oned}}\\
%   \bfigurerule}


% Render the references
\clearpage\raggedright
\bibliography{refs}

\end{document}
